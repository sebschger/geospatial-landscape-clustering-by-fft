{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45ce995",
   "metadata": {},
   "source": [
    "# Landscape Clustering by FFT Footprints\n",
    "## Primary Notebook\n",
    "\n",
    "Hi! This project is a personal project with a fun backstory. Have you ever been on vacation and wondered: Do I really have to travel this far, or is there a similar landscape closeer to my home delivering the same experience? Well I know this to be a really unusual question, but one that I asked myself, while on vacation all over Scotland.\n",
    "\n",
    "This project aims to make similar \"landscape types\" easy to discover. Essentially the notebook creates one or more GeoTIFFs where different landscape types are colored. They are clustered by their topographic footprint.\n",
    "\n",
    "### The topographic footprint\n",
    "\n",
    "\n",
    "### Overview: \n",
    "This is the primary notebook of this repository, and the one that does the heavy work.\n",
    "\n",
    "There are other notebooks to this repository:\n",
    "- secondary_dem_retriever.ipynb -> This one connects to opentopography.org and retrieves DEM geotiffs (aka heightmaps) of parts of the earth.\n",
    "- color_separator.ipynb -> This is just a little helper notebook (only rough code). This one helped creating custom color palettes for coloring the resulting images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a574dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we might need eventually\n",
    "# rasterio for geotiffs\n",
    "# dask array for parallel computing of large arrays\n",
    "# pyfftw for 2d fft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2bb14b",
   "metadata": {},
   "source": [
    "## Overview of the classes and their interconnection\n",
    "The subject of this notebook is to cluster different landscapes across multiple DEM-GeoTIFFs. \n",
    "\n",
    "### GeographicBounds\n",
    "This is an object that saves West, South, East, North borders, as well as the projection, and maybe some other additional info.\n",
    "\n",
    "### AugmentedDEM\n",
    "This will be the \"container\" for all information regarding *one* specific DEM raster map.\n",
    "\n",
    "### EmbeddingMap\n",
    "This will be part of each AugmentedDEM. Here we will save the labels created by the clustering.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485369e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc9a77bc",
   "metadata": {},
   "source": [
    "## How to make the GeoTIFF images usable by my algorithm:\n",
    "It’s important to note, that the original DEM data is in arc seconds. To avoid skewed results it has to be compressed by cos(latitude) in width.  That way the metric distances (almost) resemble the distances pixel-wise\n",
    "\n",
    "After that the images will be scaled to 1/4 (as of now), because processing becomes about 4^2 times faster.\n",
    "\n",
    "Before processing, the data will have to be split up into quadratic tiles which are supposed to be equal in their length.\n",
    "\n",
    "I guess we should combine the processes, namely selecting tiles of equal size and then reducing their size to a given size in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eaa0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Filesystem, JSON\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# Rasterio for handling GeoTIFFs\n",
    "import rasterio\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.windows import from_bounds\n",
    "\n",
    "\n",
    "# Resampling of the tiles\n",
    "from PIL import Image \n",
    "\n",
    "\n",
    "# For interpolation of the results\n",
    "from scipy.interpolate import griddata\n",
    "from scipy import ndimage # for filtering\n",
    " \n",
    "\n",
    "# Math may not be missing\n",
    "import math\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import pyfftw \n",
    "\n",
    "\n",
    "# The heart: we use k-means-clustering\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "# Regex for updating filenames\n",
    "import re\n",
    "\n",
    "\n",
    "# For earth related numbers\n",
    "from pyproj import Geod\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "\n",
    "# Matplotlib for graphics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "# For timing \n",
    "import time\n",
    "\n",
    "\n",
    "# For multiprocessing\n",
    "from multiprocess import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTimer:\n",
    "    '''This class is just a timer for checking the performance'''\n",
    "    def __init__(self, description):\n",
    "        self.description = description\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.timer = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.time_needed = time.perf_counter() - self.timer\n",
    "        print(f\"{self.description} took {self.time_needed:.1f} Seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a76d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseInfoTimer:\n",
    "    '''\n",
    "    This class wraps processes to help identifying what is done by printing info about them.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, process_description: str, current_index: int = 1, total_count: int = 1, single_description: str | None = None):\n",
    "        self.sd = single_description\n",
    "        self.pd = process_description\n",
    "        self.curr_it = current_index + 1\n",
    "        self.total_count = total_count\n",
    "        self.rest = total_count - self.curr_it\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.timer = time.perf_counter()\n",
    "\n",
    "        if self.total_count > 1:\n",
    "            if self.curr_it == 1:\n",
    "                print(f\"\\n{self.pd} starting…\\n\")\n",
    "            print(f\"{self.sd} {self.curr_it} of {self.total_count}…\")\n",
    "        else:\n",
    "            print(f\"{self.pd} starting…\")\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.time_needed = time.perf_counter() - self.timer\n",
    "\n",
    "        if self.total_count > 1:\n",
    "            print(f\"{self.sd} completed. This one took {self.time_needed:.1f} seconds.\")\n",
    "\n",
    "            if self.rest > 0:\n",
    "                print(f\"{self.rest} more to go.\")\n",
    "            else:\n",
    "                print(f\"{self.pd} ({self.total_count}) completed.\")\n",
    "        else:\n",
    "            print(f\"{self.pd} completed. It took {self.time_needed:.1f} seconds.\")\n",
    "\n",
    "        print(\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1956f9",
   "metadata": {},
   "source": [
    "<a id=\"settings\">Anchor</a>\n",
    "## Code explanation: Settings\n",
    "The following section sets the parameters for the main functions. I chose dictionaries to later be able to import and export JSON configs easily. \n",
    "\n",
    "In general there are **internal_settings** that control the execution, as well as **temporary_settings** in which the intermediary results will be stored.\n",
    "\n",
    "The execution of the processing pipeline starts [at the end of the notebook](#execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddca61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for running this Notebook\n",
    "\n",
    "internal_settings = {}\n",
    "\n",
    "internal_settings[\"files\"] = {}\n",
    "internal_settings[\"files\"][\"dem_folder\"] = \"geotiffs\" #Here the to-be-used-geotiffs are located\n",
    "\n",
    "internal_settings[\"fft\"] = {}\n",
    "internal_settings[\"fft\"][\"tile_size_km\"] = 10 #average length and width of a tile that is processed individually\n",
    "internal_settings[\"fft\"][\"tile_size_px\"] = 20\n",
    "\n",
    "internal_settings[\"fft\"][\"tile_overlap_percent\"] = 85 # 85 How much (by a tile length in percent) do the tiles overlap\n",
    "internal_settings[\"fft\"][\"tile_overlap_multi\"] =  1 / (1 - (internal_settings[\"fft\"][\"tile_overlap_percent\"]/100))\n",
    "internal_settings[\"fft\"][\"fft_levels\"] = 12  \n",
    "\n",
    "internal_settings[\"output\"] = {}\n",
    "internal_settings[\"output\"][\"folder_name\"] = \"output_label_images\" \n",
    "internal_settings[\"output\"][\"q_factor\"] = 10 # 5 Bigger Q-Factor, smaller image result, faster computation\n",
    "internal_settings[\"output\"][\"label_mode_filter\"] = 15  # this is in arbitrary units. It will adapt to the final size, so bigger images are filtered in the same way\n",
    "internal_settings[\"output\"][\"label_count\"] = 10\n",
    "\n",
    "# This one calculates the actual filter size in pixels regarding the final image\n",
    "internal_settings[\"output\"][\"label_mode_filter_radius\"] =  int(internal_settings[\"output\"][\"label_mode_filter\"] / (internal_settings[\"output\"][\"q_factor\"] / 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a946bf",
   "metadata": {},
   "source": [
    "## Code explanation: Multiprocessing\n",
    "One major step in the processing is to extract tiles of equal (metric) size from the original DEM GeoTIFF. \n",
    "As this is really processor intensive, I chose to make use of the **multiprocess** library. It really speeds things up by factor 10 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570cb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocessing stuff\n",
    "def init_worker(image_array, transform, tile_size):\n",
    "    global shared_image_array, shared_transform, shared_tile_size\n",
    "    shared_image_array, shared_transform, shared_tile_size = image_array, transform, tile_size\n",
    "\n",
    "def resample_tile(bounds):\n",
    "    global shared_image_array, shared_transform, shared_tile_size\n",
    "\n",
    "    source_window = from_bounds(*bounds, transform=shared_transform)\n",
    "\n",
    "    # Round the resulting window to actual pixels to be used as indices\n",
    "    window_row_off_px = int(source_window.row_off)\n",
    "    window_col_off_px = int(source_window.col_off)\n",
    "    window_height_px = int(source_window.height)\n",
    "    window_width_px = int(source_window.width)\n",
    "\n",
    "    # Create a tile as an np array from the original raster image using the given bounds\n",
    "    # First index is for different channels!\n",
    "    tile = shared_image_array[:, window_row_off_px : window_row_off_px + window_height_px,\n",
    "        window_col_off_px : window_col_off_px + window_width_px]\n",
    "\n",
    "    tile_resampled = np.stack([\n",
    "        np.array(Image.fromarray(channel).resize(\n",
    "            (shared_tile_size, shared_tile_size), resample=Image.BILINEAR)) \n",
    "            for channel in tile])\n",
    "\n",
    "    return tile_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort labels by size\n",
    "\n",
    "def sort_labels(labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    size_order = np.argsort(-counts)\n",
    "\n",
    "    lookup = np.empty_like(unique)\n",
    "    lookup[size_order] = np.arange(len(size_order))\n",
    "\n",
    "    relabeled = lookup[labels]\n",
    "    return relabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfba279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes specific to this project\n",
    "\n",
    "class GeographicBounds:\n",
    "    '''\n",
    "    Stores coordinates for both the dem maps and also the tiles.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, west, south, east, north):\n",
    "        self.west = west\n",
    "        self.north = north\n",
    "        self.east = east\n",
    "        self.south = south\n",
    "\n",
    "\n",
    "    # Center longitude\n",
    "    @property\n",
    "    def center_x (self): \n",
    "        return (self.west+self.east)/2\n",
    "\n",
    "\n",
    "    # Center latitude\n",
    "    @property\n",
    "    def center_y (self): \n",
    "        return (self.north+self.south)/2\n",
    "\n",
    "\n",
    "    # Center latitude\n",
    "    @property\n",
    "    def center_combined (self):\n",
    "        return (self.center_y, self.center_x)\n",
    "\n",
    "\n",
    "    # Total height in degrees\n",
    "    @property\n",
    "    def height_degrees (self): \n",
    "        return (abs(self.south - self.north))\n",
    "\n",
    "\n",
    "    # Total width in degrees\n",
    "    @property\n",
    "    def width_degrees (self): \n",
    "        return min (abs(self.east - self.west), 360 - abs(self.east - self.west));  #when getting to ±180° it should choose the shorter width # CAUTION, this might be not the final solution\n",
    "\n",
    "\n",
    "    # Total height in kilometres\n",
    "    @property\n",
    "    def height_km (self):\n",
    "        return (abs(self.south - self.north) / 360.0) * math.pi * 2.0 * geod.b / 1000.0\n",
    "\n",
    "\n",
    "    # Total width of the northern edge in kilometres\n",
    "    @property\n",
    "    def width_north_km (self):\n",
    "        return (abs(self.west - self.east) / 360.0) * math.pi * 2.0 * geod.a * math.cos( math.radians(self.north) ) / 1000.0\n",
    "\n",
    "\n",
    "    # Total width of the southern edge in kilometres\n",
    "    @property\n",
    "    def width_south_km (self):\n",
    "        return (abs(self.west - self.east) / 360.0) * math.pi * 2.0 * geod.a * math.cos( math.radians(self.south) ) / 1000.0\n",
    "\n",
    "\n",
    "    # Return information about the object (for to be used in a print statement for example)\n",
    "    def __str__(self):\n",
    "        infostring = \"Infos about the GeographicBounds (all rounded):\\n\"\n",
    "        infostring += f\"-----\\n\"\n",
    "        infostring += f\"Geographic Bounds:\\n\"\n",
    "        infostring += f\"West: {self.west:>8.2f}° \\n\"\n",
    "        infostring += f\"South: {self.south:>7.2f}° \\n\"\n",
    "        infostring += f\"East: {self.east:>8.2f}° \\n\"\n",
    "        infostring += f\"North: {self.north:>7.2f}\\n\"\n",
    "        infostring += f\"-----\\n\"\n",
    "        infostring += f\"Northern Width: {self.width_north_km:.2f} km\\n\"\n",
    "        infostring += f\"Southern Width: {self.width_south_km:.2f} km\\n\"\n",
    "        infostring += f\"Height: {self.height_km:.2f} km\\n\"\n",
    "        infostring += f\"-----\\n\"\n",
    "        infostring += f\"Center (lon E, lat N): {self.center_y:.3f}°, {self.center_x:.3f}°\\n\"\n",
    "        infostring += \"\\n\"\n",
    "        return infostring\n",
    "\n",
    "\n",
    "    def as_list(self):\n",
    "        return [self.west, self.south, self.east, self.north]\n",
    "\n",
    "    def as_list_rearranged(self):\n",
    "        return [self.west, self.north, self.east, self.south]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5496a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModeFilterLabels(label_map, radius = 10):\n",
    "    '''\n",
    "    Returns the most prevalent label in a given radius inside a label map.\n",
    "    The label map has to be in shape (y,x), the label values can be arbitrary integers.\n",
    "    '''\n",
    "    y, x = np.ogrid[-radius:radius+1, -radius:radius+1]\n",
    "    mask = x**2 + y**2 <= radius **2\n",
    "    \n",
    "\n",
    "    def mode_filter_func(values):\n",
    "        counts = np.bincount(values.astype(\"uint8\"))\n",
    "        return np.argmax(counts)\n",
    "    \n",
    "    filtered = ndimage.generic_filter(\n",
    "        label_map,\n",
    "        function = mode_filter_func,\n",
    "        footprint=mask,\n",
    "        mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AugmentedDEM:\n",
    "    '''\n",
    "    Handles the segmentation and individual processing of each GeoTIFF (aka DEM map).\n",
    "    '''\n",
    "    # Settings will be handed over before the processing begins\n",
    "    settings = None\n",
    "\n",
    "    def __init__(self, dem_path):\n",
    "        '''\n",
    "        Load key metadata from the GeoTIFF at the specified filepath.\n",
    "        '''\n",
    "\n",
    "        # Hint: No DEM data is loaded until processing.\n",
    "        with rasterio.open(dem_path) as dem_file:\n",
    "            # Save the geographic bounds in a new nested object\n",
    "            self.geo_bounds = GeographicBounds(dem_file.bounds.left, \n",
    "                                               dem_file.bounds.bottom,\n",
    "                                               dem_file.bounds.right,\n",
    "                                               dem_file.bounds.top)\n",
    "            \n",
    "            # Save the original dimensions inside this object\n",
    "            self.dem_width_px = dem_file.width\n",
    "            self.dem_height_px = dem_file.height\n",
    "        \n",
    "        # Save the original path for later \n",
    "        self.dem_path = dem_path\n",
    "        \n",
    "\n",
    "\n",
    "    def _calculate_tile_bounds(self): \n",
    "        '''\n",
    "        This partitions the dem map into tiles that are equal in physical length.\n",
    "        It sets the tile_bounds list (bounds in degrees).\n",
    "        '''\n",
    "        \n",
    "        # This is the multiplier that essentially defines the overlap of the tiles.\n",
    "        # A multiplier of 1 is no overlap, a multiplier of 2 is an overlap of 50%\n",
    "        # In the settings area the overlap is set by percent, \n",
    "        # the overlap multiplier is successively calculated from that value\n",
    "        tilemulti = self.settings[\"fft\"][\"tile_overlap_multi\"]\n",
    "\n",
    "        tile_height_degrees = 360 * self.settings[\"fft\"][\"tile_size_km\"] * 1000 /  (math.pi * 2.0 * geod.b)\n",
    "\n",
    "        relative_start_y = (self.geo_bounds.height_degrees % tile_height_degrees) / 2\n",
    "        absolute_start_y = (self.geo_bounds.north - relative_start_y)\n",
    "        absolute_end_y = (self.geo_bounds.south + relative_start_y)\n",
    "        number_tiles_y = int((self.geo_bounds.height_degrees // tile_height_degrees)*tilemulti)\n",
    "\n",
    "        absolute_borders_y = np.linspace(absolute_start_y, absolute_end_y, number_tiles_y + 1 )\n",
    "        absolute_centers_y = np.linspace(absolute_start_y - (tile_height_degrees / 2), absolute_end_y + (tile_height_degrees / 2), number_tiles_y )\n",
    "\n",
    "\n",
    "        # Here will go the longitudinal splitting. The tiles of each latitude will be split with a different tile_width_degrees for each centers_y. That is due to the fact that inside a sector of the earth’s surface is getting smaller in direction of the poles (cos). This is why this is a list of np.arrays and not a 2D np-array (as length varies for each row!)\n",
    "\n",
    "        absolute_borders_x = []\n",
    "        self.tile_bounds = []\n",
    "\n",
    "        \n",
    "        for i, center_y in enumerate(absolute_centers_y):\n",
    "            tile_width_degrees = 360 * self.settings[\"fft\"][\"tile_size_km\"] * 1000 /  (math.pi * 2.0 * geod.a)  / math.cos (math.radians(center_y))\n",
    "            \n",
    "            relative_start_x = (self.geo_bounds.width_degrees % tile_width_degrees) / 2\n",
    "            absolute_start_x = (self.geo_bounds.west + relative_start_x)\n",
    "            absolute_end_x = (self.geo_bounds.east - relative_start_x)\n",
    "            number_tiles_x = int((self.geo_bounds.width_degrees // tile_width_degrees)*tilemulti)\n",
    "\n",
    "            tmp_absolute_borders_x = np.linspace(absolute_start_x, absolute_end_x, number_tiles_x + 1 )\n",
    "            tmp_absolute_centers_x = np.linspace(absolute_start_x + (tile_width_degrees / 2), absolute_end_x - (tile_width_degrees / 2), number_tiles_x)\n",
    "\n",
    "            absolute_borders_x.append( np.array(tmp_absolute_borders_x) )\n",
    "\n",
    "        \n",
    "        for y in range(len(absolute_centers_y-1)):\n",
    "            for x in range(len(absolute_borders_x[y])-1):\n",
    "                self.tile_bounds.append(\n",
    "                    GeographicBounds(absolute_borders_x[y][x],\n",
    "                                    absolute_borders_y[y],\n",
    "                                    absolute_borders_x[y][x+1],\n",
    "                                    absolute_borders_y[y+1]))\n",
    "                \n",
    "\n",
    "\n",
    "    def _resample_tiles_multiprocessing(self):\n",
    "        '''\n",
    "        This method resamples tiles from the original dem data\n",
    "        to all have the same pixel dimensions.\n",
    "        '''\n",
    "    \n",
    "        tile_size = self.settings[\"fft\"][\"tile_size_px\"]\n",
    "\n",
    "        with SimpleTimer(\"Multiprocessing Resample of Tiles\"):\n",
    "         \n",
    "            \n",
    "            with rasterio.open(self.dem_path) as source_image:\n",
    "                full_image_array = source_image.read()\n",
    "                transform = source_image.transform\n",
    "            \n",
    "            tile_bounds_list = [bnd.as_list_rearranged() for bnd in self.tile_bounds]\n",
    "\n",
    "            with Pool(processes=cpu_count(),\n",
    "                    initializer=init_worker,\n",
    "                    initargs=(full_image_array, transform, tile_size)) as pool:\n",
    "                self.tiles_resampled = np.array(pool.map(resample_tile, tile_bounds_list))\n",
    "                self.tiles_resampled = np.squeeze(self.tiles_resampled, axis = 1) \n",
    "                # remove the dimension for the channels, as in DEM data there should only be one channel\n",
    "\n",
    "               \n",
    "    def dump_tiles(self):\n",
    "        '''This method writes the resampled tiles to disk, for testing.'''\n",
    "\n",
    "        for i, tile in enumerate(self.tiles_resampled):\n",
    "            height = self.settings[\"fft\"][\"tile_size_px\"]\n",
    "            width = self.settings[\"fft\"][\"tile_size_px\"]\n",
    "\n",
    "            # Calculate pixel resolution (degrees per pixel)\n",
    "            res_x = ((self.tile_bounds[i].east - self.tile_bounds[i].west) / width)\n",
    "            res_y = ((self.tile_bounds[i].north - self.tile_bounds[i].south) / height)\n",
    "\n",
    "            # Create affine transform (pixel coordinates → WGS84)\n",
    "            transform = Affine.translation(self.tile_bounds[i].west, self.tile_bounds[i].south) * Affine.scale(res_x, res_y)\n",
    "\n",
    "            filepath = f\"tile_dump/{self.tile_bounds[i].west:.3f}_{self.tile_bounds[i].north:.3f}.tif\"\n",
    "\n",
    "            # Write GeoTIFF\n",
    "            with rasterio.open(\n",
    "                filepath,\n",
    "                \"w\",\n",
    "                driver=\"GTiff\",\n",
    "                height=height,\n",
    "                width=width,\n",
    "                count=1, \n",
    "                dtype=tile.dtype,\n",
    "                crs=\"EPSG:4326\", \n",
    "                transform=transform,\n",
    "            ) as dst:\n",
    "                dst.write(tile[np.newaxis,:])\n",
    "\n",
    "\n",
    "\n",
    "    def _create_fft_tiles(self):\n",
    "        '''\n",
    "        This creates an 2D FFT magnitude map for each individual tile of the map.\n",
    "        '''\n",
    "        fft_input_array = pyfftw.empty_aligned((len(self.tiles_resampled),\n",
    "                                                self.settings[\"fft\"][\"tile_size_px\"],\n",
    "                                                self.settings[\"fft\"][\"tile_size_px\"]),\n",
    "                                                dtype = \"complex64\")\n",
    "\n",
    "        fft_output_array =  pyfftw.empty_aligned((len(self.tiles_resampled),\n",
    "                                                self.settings[\"fft\"][\"tile_size_px\"],\n",
    "                                                self.settings[\"fft\"][\"tile_size_px\"]),\n",
    "                                                dtype = \"complex64\")\n",
    "\n",
    "        fft_execution_plan = pyfftw.FFTW(fft_input_array,\n",
    "                                        fft_output_array, axes=(1,2),\n",
    "                                        direction=\"FFTW_FORWARD\",\n",
    "                                        flags=(\"FFTW_MEASURE\",))\n",
    "\n",
    "        fft_input_array[:] = self.tiles_resampled #Important. [:] ensures the reserved empty array is used, and no new one is created! It will not work without that special slicing.\n",
    "\n",
    "        fft_execution_plan.execute()\n",
    "\n",
    "        fft_magnitude_spectra_unshifted = np.log(np.abs(fft_output_array) + 1) \n",
    "        self.fft_footprint = np.fft.fftshift(fft_magnitude_spectra_unshifted, axes=(1,2)) #Centered FFT\n",
    "\n",
    "\n",
    "\n",
    "    def _split_and_average_fft_tiles(self):\n",
    "        '''\n",
    "        This splits the FFT magnitude map for each tile into parts that average in a given distance.\n",
    "        This is essentially the \"FFT Footprint\".\n",
    "        The result will be a numpy array in the shape of (number_of_tiles, number_of_fft_levels)\n",
    "        '''\n",
    "\n",
    "        # Set up Dask Arrays for faster computation of the weighted averages\n",
    "\n",
    "\n",
    "        # The general dimensions are:\n",
    "        # 1. Number of tiles (total)\n",
    "        # 2. Height of a tile\n",
    "        # 3. Width of a tile\n",
    "        # 4. Number of levels (circle filters)\n",
    "\n",
    "        # To multiply and divide all the dimensions have to be in the same order for all arrays\n",
    "\n",
    "        # Weights – here go the circles\n",
    "        # Weights are still (#4, #2, #3), change them to #2, #3, #4\n",
    "\n",
    "\n",
    "        with SimpleTimer(\"Creating the weighted averages per mask\"):\n",
    "            da_weights = da.from_array(\n",
    "                # The height becomes axis 0, width becomes axis 1, and different masks become axis 2\n",
    "                np.transpose(temporary_settings[\"circle_masks\"],(1,2,0)) \n",
    "            )\n",
    "\n",
    "            # First dimension (#1) is missing, add it \n",
    "            # Just a note: the tiles themselves are not x-y adressed, but the get a continuous index (1D)\n",
    "            da_weights = da_weights[np.newaxis, ...]\n",
    "            # Now the form of the weights is (#1, #2, #3, #4) like explained above\n",
    "\n",
    "            # Take the results of the fft and place them also in a dask array\n",
    "            # of the wanted form (see above) \n",
    "            da_magnitude_spectra = da.from_array(self.fft_footprint)[...,np.newaxis]\n",
    "\n",
    "            # Sum the values of the areas covered by the circles, to later have something to divide by\n",
    "            # to generate weighted sums\n",
    "            da_weights_sum = da.sum(da_weights, axis=(1,2)) \n",
    "\n",
    "            # Sum the results of the FFTs multiplied by the weights of the different levels (circle filters)\n",
    "            da_sum_of_spectra = da.sum(da_magnitude_spectra * da_weights, axis=(1, 2))\n",
    "\n",
    "            # Calculate the weighted average \n",
    "            da_weighted_averages = da_sum_of_spectra / da_weights_sum\n",
    "\n",
    "            # The result is now in the shape of #1, #4 -> Number of Tiles, Number of Levels\n",
    "            self.fft_magnitude_all_levels = da_weighted_averages.compute() \n",
    "\n",
    "    \n",
    "    def create_image(self):\n",
    "        '''\n",
    "        This method creates a colored image. \n",
    "        After the clustering has taken place the found labels will be translated to colors for each pixel.\n",
    "        '''\n",
    "\n",
    "        # Each tile has gotten a label\n",
    "        # We know the center of each tile\n",
    "        # That way we can interpolate an image by combining the two data sources\n",
    "        # Remember the tiles are not in a strict raster, because they become taller\n",
    "        # the closer they get to the equator.\n",
    "\n",
    "        tmp_tile_positions = [tile.center_combined for tile in self.tile_bounds]\n",
    "        tmp_pic_width = self.dem_width_px // self.settings[\"output\"][\"q_factor\"]\n",
    "        tmp_pic_height = self.dem_height_px // self.settings[\"output\"][\"q_factor\"]\n",
    "\n",
    "        tmp_grid_res_y = np.linspace(self.geo_bounds.north, self.geo_bounds.south, tmp_pic_height )\n",
    "        tmp_grid_res_x = np.linspace(self.geo_bounds.west, self.geo_bounds.east, tmp_pic_width )\n",
    "\n",
    "        tmp_grid_y, tmp_grid_x = np.meshgrid(tmp_grid_res_y, tmp_grid_res_x, indexing=\"ij\")\n",
    "\n",
    "        tmp_interpolated = griddata(tmp_tile_positions,\n",
    "        self.labels, (tmp_grid_y, tmp_grid_x), method='nearest') \n",
    "\n",
    "\n",
    "        with SimpleTimer(\"Filtering of the mapped labels\"):\n",
    "            tmp_interpolated = ModeFilterLabels(tmp_interpolated, self.settings[\"output\"][\"label_mode_filter_radius\"])\n",
    "\n",
    "\n",
    "\n",
    "        # Start creating the image\n",
    "   \n",
    "        # I create my own color mapping for I like those colors\n",
    "        custom_colors_str = [\"#053608\" ,\n",
    "                            \"#bdf2e6\" ,\n",
    "                            \"#1c5532\" ,\n",
    "                            \"#62f8ee\" ,\n",
    "                            \"#ffba20\" ,\n",
    "                            \"#2c7eb5\" ,\n",
    "                            \"#b9c5d6\" ,\n",
    "                            \"#404383\" ,\n",
    "                            \"#91f88f\" ,\n",
    "                            \"#759d7c\" ,\n",
    "                            \"#9395a3\"  \n",
    "        ]\n",
    "\n",
    "        # The image becomes colors mapped from the custom colors indexed by the constrained labels\n",
    "        tmp_labels_image_rgb = color_to_numpy(custom_colors_str)[constrain_labels(tmp_interpolated)]\n",
    "        self.image_rgb = (np.transpose(tmp_labels_image_rgb, (2,0,1))).astype(\"uint8\")\n",
    "\n",
    "\n",
    "    def write_image(self):\n",
    "        '''Writes the created image to the disk.'''\n",
    "\n",
    "        tmp_pic_width = self.dem_width_px // self.settings[\"output\"][\"q_factor\"]\n",
    "        tmp_pic_height = self.dem_height_px // self.settings[\"output\"][\"q_factor\"]\n",
    "\n",
    "        # Calculate pixel resolution for the GeoTIFF output image (degrees per pixel)\n",
    "        res_x = ((self.geo_bounds.east - self.geo_bounds.west) / tmp_pic_width)\n",
    "        res_y = ((self.geo_bounds.south - self.geo_bounds.north) / tmp_pic_height)\n",
    "\n",
    "        # Create affine transform (pixel coordinates → WGS84)\n",
    "        transform = Affine.translation(self.geo_bounds.west,\n",
    "                                    self.geo_bounds.north) * Affine.scale(res_x, res_y)\n",
    "\n",
    "        # I put together a comprehensive (very long) filename to recall settings later\n",
    "        filename_string = os.path.basename(self.dem_path) + \\\n",
    "        f\"tlszkm {self.settings[\"fft\"][\"tile_size_km\"]:.1f}  \" + \\\n",
    "        f\"tlszpx {self.settings[\"fft\"][\"tile_size_px\"]:.0f}  \" + \\\n",
    "        f\"fftlvls {self.settings[\"fft\"][\"fft_levels\"]:.0f}  \" + \\\n",
    "        f\"qfctr {self.settings[\"output\"][\"q_factor\"]:.0f}  \" + \\\n",
    "        f\"{self.settings[\"fft\"][\"tile_overlap_multi\"]:.1f}x\"+ \\\n",
    "        f\"-ovrlp-pct {self.settings[\"fft\"][\"tile_overlap_percent\"]:.0f} \"+ \\\n",
    "        f\"fltrrds {self.settings[\"output\"][\"label_mode_filter_radius\"]:.0f}\"+ \\\n",
    "        f\"lblct {self.settings[\"output\"][\"label_count\"]:.0f}\"+ \\\n",
    "        f\".tif\"\n",
    "\n",
    "        filepath = os.path.join(self.settings[\"output\"][\"folder_name\"], filename_string)\n",
    "\n",
    "        # Write GeoTIFF\n",
    "        with rasterio.open(\n",
    "            filepath,\n",
    "            \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=tmp_pic_height,\n",
    "            width=tmp_pic_width,\n",
    "            count=3, \n",
    "            dtype=\"uint8\",\n",
    "            crs=\"EPSG:4326\", \n",
    "            transform=transform,\n",
    "        ) as dst:\n",
    "            dst.write(self.image_rgb[0],1)\n",
    "            dst.write(self.image_rgb[1],2)\n",
    "            dst.write(self.image_rgb[2],3)\n",
    "\n",
    "\n",
    "    def process_dem(self):\n",
    "        '''\n",
    "        This method runs all the necessary steps of the processing, \n",
    "        except for clustering and creating the image.\n",
    "        Remember: Clustering has to be done across all loaded DEM maps\n",
    "        to create comparable clusters.\n",
    "        '''\n",
    "\n",
    "        # If we did that per DEM map and not globally, we would still get clusters\n",
    "        # However they would not correspond to the clusters in the other DEM maps. \n",
    "\n",
    "        self._calculate_tile_bounds()\n",
    "\n",
    "        self._resample_tiles_multiprocessing()\n",
    "\n",
    "        with SimpleTimer(\"Creating FFT footprints\"):\n",
    "            self._create_fft_tiles()\n",
    "\n",
    "        self._split_and_average_fft_tiles()\n",
    "\n",
    "\n",
    "        # Hereafter the clustering takes place (global via class method)\n",
    "        # After that each colored label map is written to the disk.\n",
    "\n",
    "    @classmethod\n",
    "    def create_labels(cls, instances):\n",
    "        '''\n",
    "        Create one long numpy array from all the tile’s labels of every given dem map.\n",
    "        '''\n",
    "         \n",
    "        all_magnitudes_np = np.concatenate([d.fft_magnitude_all_levels for d in instances], axis = 0)\n",
    "\n",
    "        # Normalize across all DEMs\n",
    "        all_magnitudes_np = (all_magnitudes_np - np.median(all_magnitudes_np, axis=0, keepdims=True)) / np.std(all_magnitudes_np, axis=0, keepdims=True)\n",
    "\n",
    "        # Store the original sizes to late split the labels again\n",
    "        original_tile_count = [d.fft_magnitude_all_levels.shape[0] for d in instances]\n",
    "        original_indices_for_splitting =  np.cumsum(original_tile_count[:-1])\n",
    "\n",
    "        # Clustering section\n",
    "        n_clusters = cls.settings[\"output\"][\"label_count\"]\n",
    "        k_means_clusterer = KMeans(n_clusters = n_clusters, random_state=550)\n",
    "\n",
    "\n",
    "        with SimpleTimer(\"Clustering\"):\n",
    "                all_dem_labels = k_means_clusterer.fit_predict(all_magnitudes_np)\n",
    "\n",
    "        with SimpleTimer(\"Relabeling Cluster IDs by size\"):\n",
    "                all_dem_labels = sort_labels(all_dem_labels)\n",
    "\n",
    "        # Distribute the results back to the augmented dem objects\n",
    "        single_dem_lables = np.split(all_dem_labels, original_indices_for_splitting) \n",
    "\n",
    "        for i, labels in enumerate(single_dem_lables):\n",
    "            instances[i].labels = labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646078e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def constrain_labels(input):\n",
    "    '''\n",
    "    Takes a list of integer labels in any range and converts them to 1, 2, 3…\n",
    "    '''\n",
    "\n",
    "    unique_labels = np.sort(np.unique(input))\n",
    "    new_labels = np.arange(0,len(unique_labels), 1)\n",
    "    from_to = dict(zip(unique_labels,new_labels))\n",
    "\n",
    "    constrained_labels = np.vectorize(from_to.get)(input)\n",
    "\n",
    "    return constrained_labels\n",
    "\n",
    "def color_to_numpy(stringlist):\n",
    "    '''\n",
    "    This converts a list of string hex color codes to actual rgb values in a numpy array.\n",
    "    '''\n",
    "\n",
    "    colorlist = np.empty((len(stringlist),3))\n",
    "\n",
    "    for i, string in enumerate(stringlist):\n",
    "        removed_hashtag = re.search(r\"(?i)([a-f0-9]+)\", string).group(0)\n",
    "        colorlist[i] = np.array((int(removed_hashtag[0:2],16),\n",
    "                        int(removed_hashtag[2:4],16),\n",
    "                        int(removed_hashtag[4:6],16)))\n",
    "\n",
    "    return colorlist\n",
    "\n",
    "def euclidean_distance (y_a, y_b, x_a, x_b):\n",
    "    '''\n",
    "    Calculates the pythagorean distance between two points.\n",
    "    '''\n",
    "\n",
    "    return math.sqrt((y_a-y_b)**2 + (x_a-x_b)**2)\n",
    "\n",
    "def threshold (input, threshold, bandwidth = 1):\n",
    "    '''\n",
    "    Creates smooth edges in the circle masks.\n",
    "    Values below threshold become 0, values above 1.\n",
    "    Values at the threshold ± half bandwidth are faded.\n",
    "    '''\n",
    "\n",
    "    # This is essentially anti aliasing without subsampling.\n",
    "\n",
    "    result = np.interp(input, [threshold-(bandwidth/2), threshold+(bandwidth/2)], [0,1])\n",
    "    return result;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30601272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper classes\n",
    "\n",
    "def CircleImage (height, width, radius, inverted = False, bandwidth = 1):\n",
    "    # Returns the image of a circle as a numpy array for masking\n",
    "    # The bandwidth defines the width of the border of the circle (for anti aliasing)\n",
    "\n",
    "    circle_image = np.zeros((height, width))\n",
    "\n",
    "    if radius == 0:\n",
    "        return (1 - circle_image) if inverted else (circle_image)\n",
    "    else:\n",
    "        for x in range(width):\n",
    "            for y in range(height):\n",
    "                circle_image[y,x] = euclidean_distance(\n",
    "                    y+(0.5 if height%2 == 1 else 0), height / 2,\n",
    "                    x+(0.5 if width%2 == 1 else 0), width / 2\n",
    "                )\n",
    "\n",
    "    if inverted:\n",
    "        return 1-threshold(circle_image, radius, 1)\n",
    "    else:\n",
    "        return threshold(circle_image, radius, 1)\n",
    "\n",
    "def RingImage (height, width, inner_radius, outer_radius, bandwidth):\n",
    "    if outer_radius < inner_radius:\n",
    "        raise ValueError(\"The inner radius must be smaller than the outer radius.\")\n",
    "\n",
    "    # This combines two circles to form a ring mask\n",
    "    outercircle = CircleImage(height, width, outer_radius, \n",
    "                              inverted = True, bandwidth=bandwidth)\n",
    "    innercircle = CircleImage(height, width, inner_radius, \n",
    "                              inverted = True, bandwidth=bandwidth)\n",
    "    \n",
    "    ringimg = outercircle - innercircle\n",
    "\n",
    "    return  (ringimg - ringimg.min()) / (ringimg.max() - ringimg.min())\n",
    "\n",
    "def RingImageSeries(height,width,steps,bandwidth):\n",
    "    '''This creates a 3D numpy array of the shape\n",
    "    (Masks, individual Height, individual Width)\n",
    "    This is used to sum and average the FFT magnitudes'''\n",
    "\n",
    "    # The diameter gets bigger logarithmically, starting with a diameter of 1\n",
    "    smallest_side = min(height,width)\n",
    "\n",
    "    outer_radii = np.logspace(0, np.log2(smallest_side / 2), steps, base=2) # 0 means it starts with 1 (log)\n",
    "    inner_radii = np.append(0,outer_radii[:-1])\n",
    "\n",
    "\n",
    "    all_masks = np.zeros((steps,height, width))\n",
    "\n",
    "    for i in range(steps):\n",
    "        all_masks[i] = RingImage(height,width,inner_radii[i],outer_radii[i],bandwidth=bandwidth)\n",
    "            \n",
    "    return all_masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3596b",
   "metadata": {},
   "source": [
    "<a id=\"execution\">Anchor</a>\n",
    "\n",
    "## Code explanation: Processing pipeline\n",
    "\n",
    "The following cell executes all of the processing.\n",
    "Settings are set at the [top of the notebook](#settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fcc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the processing begin\n",
    "\n",
    "temporary_settings = {}\n",
    "temporary_settings[\"augmented_dems\"] = []\n",
    "\n",
    "# Assign the settings to the class variable of AugmentedDem\n",
    "AugmentedDEM.settings = internal_settings\n",
    "\n",
    "# Create objects (AugmentedDem) for all .tif-Files inside the dem_folder\n",
    "with os.scandir(internal_settings[\"files\"][\"dem_folder\"]) as dirlist:\n",
    "    temporary_settings[\"augmented_dems\"] =[AugmentedDEM(d.path) \n",
    "                                           for d in dirlist if re.search(r\"\\.tif$\",d.name)]\n",
    "    \n",
    "\n",
    "# Set up the masks for the weighted average\n",
    "with VerboseInfoTimer(\"Creating the ring masks\"):\n",
    "    temporary_settings[\"circle_masks\"] = RingImageSeries(internal_settings[\"fft\"][\"tile_size_px\"],\n",
    "                                                        internal_settings[\"fft\"][\"tile_size_px\"],\n",
    "                                                        internal_settings[\"fft\"][\"fft_levels\"],\n",
    "                                                        bandwidth = 1)\n",
    "\n",
    "\n",
    "for i, d in enumerate(temporary_settings[\"augmented_dems\"]):\n",
    "    with VerboseInfoTimer(\"Processing of all DEM maps\", \n",
    "                       current_index = i, \n",
    "                       total_count = len(temporary_settings[\"augmented_dems\"]),\n",
    "                       single_description = \"Processing DEM map\"):\n",
    "        d.process_dem()\n",
    "\n",
    "\n",
    "# Create_labels(temporary_settings[\"augmented_dems\"], internal_settings)\n",
    "with VerboseInfoTimer(\"Clustering and creating labels for all DEM maps\"):\n",
    "    AugmentedDEM.create_labels(temporary_settings[\"augmented_dems\"])\n",
    "\n",
    "\n",
    "for i, d in enumerate(temporary_settings[\"augmented_dems\"]):\n",
    "    with VerboseInfoTimer(\"Creating labeled images for all regions\", \n",
    "                       current_index = i, \n",
    "                       total_count = len(temporary_settings[\"augmented_dems\"]),\n",
    "                       single_description = \"Creating labeled image\"):\n",
    "        d.create_image()\n",
    "        d.write_image()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e9074",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary_settings[\"augmented_dems\"][0].image_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3726fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "current_augmented_dem = temporary_settings[\"augmented_dems\"][0]\n",
    "image = np.transpose(current_augmented_dem.image_rgb,[1,2,0])\n",
    "\n",
    "west, south, east, north = current_augmented_dem.geo_bounds.as_list()\n",
    "extent = (west, east, south, north)\n",
    "\n",
    "# Start plotting\n",
    "fig = plt.figure(figsize=(10, 15), dpi=256)\n",
    "ax = plt.axes(projection=ccrs.Orthographic(central_longitude=current_augmented_dem.geo_bounds.center_x, central_latitude=current_augmented_dem.geo_bounds.center_y))\n",
    "\n",
    "ax.set_extent(extent)\n",
    "\n",
    "# Add features\n",
    "ax.add_feature(cfeature.BORDERS, edgecolor = \"black\", linewidth=0.5)\n",
    "ax.add_feature(cfeature.COASTLINE, edgecolor = \"white\", linewidth=1.0)\n",
    "ax.add_feature(cfeature.LAKES, edgecolor='black', facecolor='none')\n",
    "ax.add_feature(cfeature.RIVERS, edgecolor='blue', linewidth=0.3)\n",
    "ax.add_feature(cfeature.OCEAN, zorder=1, facecolor='#102d2d')\n",
    "\n",
    "# Plot image (projecting from PlateCarree, which assumes lat/lon coordinates)\n",
    "ax.imshow(image, origin='upper', extent=extent, transform=ccrs.PlateCarree())\n",
    "\n",
    "# Optionally, add gridlines\n",
    "ax.gridlines(draw_labels=True, linewidth=0.2)\n",
    "\n",
    "# Show\n",
    "plt.title(f\"Colored landscape clusters for {os.path.basename(current_augmented_dem.dem_path)}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a7a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
